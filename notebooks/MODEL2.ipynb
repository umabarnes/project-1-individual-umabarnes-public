{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 07:49:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/08/26 07:49:31 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Modeling\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+----+----+---------+------------+----------+-----------+\n",
      "|      date|hour|count|wnd| tmp| dew|bus_count|subway_count|is_weekday|day_of_week|\n",
      "+----------+----+-----+---+----+----+---------+------------+----------+-----------+\n",
      "|2023-07-01|   0|33809|0.0|23.9|13.3|        1|           5|     false|          7|\n",
      "|2023-07-01|   1|26914|0.0|23.3|13.3|        1|           7|     false|          7|\n",
      "|2023-07-01|   2|21115|0.0|23.3|12.8|        2|           2|     false|          7|\n",
      "|2023-07-01|   3|17051|3.1|22.8|12.8|        0|           1|     false|          7|\n",
      "|2023-07-01|   4|14159|1.5|22.8|11.7|        0|          11|     false|          7|\n",
      "+----------+----+-----+---+----+----+---------+------------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to the CSV file\n",
    "csv_file_path = \"../data/curated/merged2.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 07:49:37 WARN Instrumentation: [81ca52af] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+------------------+\n",
      "|      date|hour|count|        prediction|\n",
      "+----------+----+-----+------------------+\n",
      "|2023-07-01|   0|33809|17069.311853624724|\n",
      "|2023-07-01|   1|26914|17873.481476657846|\n",
      "|2023-07-01|   2|21115| 19698.05298052082|\n",
      "|2023-07-01|   3|17051|20960.259688865117|\n",
      "|2023-07-01|   4|14159|20509.410333931002|\n",
      "|2023-07-01|   5|11827|21793.663967435095|\n",
      "|2023-07-01|   6|13210| 23741.98332639597|\n",
      "|2023-07-01|   7|15708|24245.959629329038|\n",
      "|2023-07-01|   8|19051| 26316.11233073243|\n",
      "|2023-07-01|   9|22786|26625.430049987554|\n",
      "|2023-07-01|  10|24856|27055.266163720684|\n",
      "|2023-07-01|  11|27103| 28092.24309857514|\n",
      "|2023-07-01|  12|28176|  29811.1973865615|\n",
      "|2023-07-01|  13|28834| 30342.02369699465|\n",
      "|2023-07-01|  14|30289|31188.321943614326|\n",
      "|2023-07-01|  15|31437| 32633.86798500876|\n",
      "|2023-07-01|  16|31861| 33484.85818479698|\n",
      "|2023-07-01|  17|33489| 34598.93521436062|\n",
      "|2023-07-01|  18|35206| 36026.73445585126|\n",
      "|2023-07-01|  19|35360|36772.491279080365|\n",
      "+----------+----+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Coefficients: [1017.9834071508772,163.09579137399388,-46.27019176594436,202.64834876879735,-120.78794958866025,-4561.098032666934,97.57699769036043]\n",
      "Intercept: 17893.421852172774\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Assemble the features into a single vector column named \"features\"\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"hour\", \"wnd\", \"tmp\", \"bus_count\", \"subway_count\",\"is_weekday\", \"day_of_week\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Define the Lasso regression model\n",
    "lasso = LinearRegression(featuresCol=\"features\", labelCol=\"count\", elasticNetParam=1.0)\n",
    "\n",
    "# Create a pipeline with the assembler and the Lasso model\n",
    "pipeline = Pipeline(stages=[assembler, lasso])\n",
    "\n",
    "# Fit the model to the data\n",
    "model = pipeline.fit(df)\n",
    "\n",
    "# Make predictions on the same DataFrame\n",
    "predictions = model.transform(df)\n",
    "\n",
    "# Show the predictions alongside the original data\n",
    "predictions.select(\"date\", \"hour\", \"count\", \"prediction\").show()\n",
    "\n",
    "# Optionally, view the model coefficients and intercept\n",
    "linear_model = model.stages[-1]  # The last stage is the linear regression model\n",
    "print(\"Coefficients: \" + str(linear_model.coefficients))\n",
    "print(\"Intercept: \" + str(linear_model.intercept))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 5588.473711190105\n",
      "Mean Squared Error (MSE): 53945480.08331003\n",
      "Root Mean Squared Error (RMSE): 7344.7586810806815\n",
      "R-squared (R²): 0.5141583721799545\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Evaluate the model using different metrics\n",
    "evaluator = RegressionEvaluator(labelCol=\"count\", predictionCol=\"prediction\")\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse = evaluator.evaluate(predictions, {evaluator.metricName: \"mse\"})\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# R-squared (R²)\n",
    "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "print(f\"R-squared (R²): {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 7863.158818957534\n",
      "Mean Squared Error (MSE): 94467597.39571145\n",
      "Root Mean Squared Error (RMSE): 9719.444294593774\n",
      "R-squared (R²): 0.14920969793758843\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Assemble the features into a single vector\n",
    "feature_columns = ['wnd', 'tmp', 'dew', 'atm', 'bus_count', 'subway_count', 'is_weekday', 'day_of_week']\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "df_with_features = assembler.transform(df)\n",
    "\n",
    "# Initialize Lasso linear regression model\n",
    "lasso = LinearRegression(labelCol='count', featuresCol='features', regParam=0.1, elasticNetParam=1.0)\n",
    "\n",
    "# Fit the model\n",
    "lasso_model = lasso.fit(df_with_features)\n",
    "\n",
    "# Make predictions\n",
    "predictions = lasso_model.transform(df_with_features)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol='count', predictionCol='prediction', metricName='r2')\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "mse = evaluator.evaluate(predictions, {evaluator.metricName: \"mse\"})\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R-squared (R²): {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Mean Absolute Error (MAE): 7863.145888526261\n",
      "Lasso Mean Squared Error (MSE): 94467597.03789735\n",
      "Lasso Root Mean Squared Error (RMSE): 9719.444276186645\n",
      "Lasso R-squared (R²): 0.14920970116011956\n",
      "Random Forest Mean Absolute Error (MAE): 7454.456779797125\n",
      "Random Forest Mean Squared Error (MSE): 85782247.53110453\n",
      "Random Forest Root Mean Squared Error (RMSE): 9261.870628069933\n",
      "Random Forest R-squared (R²): 0.22743134894320727\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "\n",
    "# Feature Engineering\n",
    "# Assemble features into a vector\n",
    "feature_columns = ['wnd', 'tmp', 'dew', 'atm', 'bus_count', 'subway_count', 'is_weekday', 'day_of_week']\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "df_with_features = assembler.transform(df)\n",
    "\n",
    "# Optional: Standardize features\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaled_features', withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(df_with_features)\n",
    "df_scaled = scaler_model.transform(df_with_features)\n",
    "\n",
    "# Define and train Lasso Linear Regression model with hyperparameter tuning\n",
    "lasso = LinearRegression(labelCol='count', featuresCol='scaled_features', elasticNetParam=1.0)  # Lasso with elasticNetParam=1.0\n",
    "\n",
    "# Create a parameter grid for tuning\n",
    "paramGrid = ParamGridBuilder().addGrid(lasso.regParam, [0.01, 0.1, 1.0]).build()\n",
    "\n",
    "# Create a CrossValidator\n",
    "crossval = CrossValidator(estimator=lasso,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=RegressionEvaluator(labelCol='count', predictionCol='prediction'),\n",
    "                           numFolds=5)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters\n",
    "cv_model = crossval.fit(df_scaled)\n",
    "cv_predictions = cv_model.transform(df_scaled)\n",
    "\n",
    "# Evaluate the Lasso model\n",
    "cv_evaluator = RegressionEvaluator(labelCol='count', predictionCol='prediction', metricName='r2')\n",
    "r2 = cv_evaluator.evaluate(cv_predictions)\n",
    "mae = cv_evaluator.evaluate(cv_predictions, {cv_evaluator.metricName: \"mae\"})\n",
    "mse = cv_evaluator.evaluate(cv_predictions, {cv_evaluator.metricName: \"mse\"})\n",
    "rmse = cv_evaluator.evaluate(cv_predictions, {cv_evaluator.metricName: \"rmse\"})\n",
    "\n",
    "print(f\"Lasso Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Lasso Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Lasso Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Lasso R-squared (R²): {r2}\")\n",
    "\n",
    "# Alternative Model: Random Forest Regressor\n",
    "rf = RandomForestRegressor(labelCol='count', featuresCol='scaled_features')\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = rf.fit(df_scaled)\n",
    "rf_predictions = rf_model.transform(df_scaled)\n",
    "\n",
    "# Evaluate Random Forest model\n",
    "rf_evaluator = RegressionEvaluator(labelCol='count', predictionCol='prediction', metricName='r2')\n",
    "rf_r2 = rf_evaluator.evaluate(rf_predictions)\n",
    "rf_mae = rf_evaluator.evaluate(rf_predictions, {rf_evaluator.metricName: \"mae\"})\n",
    "rf_mse = rf_evaluator.evaluate(rf_predictions, {rf_evaluator.metricName: \"mse\"})\n",
    "rf_rmse = rf_evaluator.evaluate(rf_predictions, {rf_evaluator.metricName: \"rmse\"})\n",
    "\n",
    "print(f\"Random Forest Mean Absolute Error (MAE): {rf_mae}\")\n",
    "print(f\"Random Forest Mean Squared Error (MSE): {rf_mse}\")\n",
    "print(f\"Random Forest Root Mean Squared Error (RMSE): {rf_rmse}\")\n",
    "print(f\"Random Forest R-squared (R²): {rf_r2}\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.4 ('.venv1': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc19f976deae78a4b8dd1c0db37710e451de00dcd2e448b10d5869a2fb538e25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
