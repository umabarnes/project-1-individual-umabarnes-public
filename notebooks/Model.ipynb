{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/UNI/project-1-individual-umabarnes/.venv1/lib/python3.12/site-packages/py4j/clientserver.py:503\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket\u001b[39m.\u001b[39;49msendall(command\u001b[39m.\u001b[39;49mencode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    504\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/UNI/project-1-individual-umabarnes/.venv1/lib/python3.12/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n",
      "File \u001b[0;32m~/Desktop/UNI/project-1-individual-umabarnes/.venv1/lib/python3.12/site-packages/py4j/clientserver.py:506\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    505\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mError while sending or receiving.\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JNetworkError(\n\u001b[1;32m    507\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mError while sending\u001b[39m\u001b[39m\"\u001b[39m, e, proto\u001b[39m.\u001b[39mERROR_ON_SEND)\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m: Error while sending",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Create a Spark session\u001b[39;00m\n\u001b[1;32m     10\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mModeling\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/UNI/project-1-individual-umabarnes/.venv1/lib/python3.12/site-packages/pyspark/sql/session.py:503\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    500\u001b[0m     session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n\u001b[1;32m    501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     \u001b[39mgetattr\u001b[39m(\n\u001b[0;32m--> 503\u001b[0m         \u001b[39mgetattr\u001b[39;49m(session\u001b[39m.\u001b[39;49m_jvm, \u001b[39m\"\u001b[39;49m\u001b[39mSparkSession$\u001b[39;49m\u001b[39m\"\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39mMODULE$\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m     )\u001b[39m.\u001b[39mapplyModifiableSettings(session\u001b[39m.\u001b[39m_jsparkSession, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n\u001b[1;32m    505\u001b[0m \u001b[39mreturn\u001b[39;00m session\n",
      "File \u001b[0;32m~/Desktop/UNI/project-1-individual-umabarnes/.venv1/lib/python3.12/site-packages/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m UserHelpAutoCompletion\u001b[39m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[39mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client\u001b[39m.\u001b[39;49msend_command(\n\u001b[1;32m   1713\u001b[0m     proto\u001b[39m.\u001b[39;49mREFLECTION_COMMAND_NAME \u001b[39m+\u001b[39;49m\n\u001b[1;32m   1714\u001b[0m     proto\u001b[39m.\u001b[39;49mREFL_GET_UNKNOWN_SUB_COMMAND_NAME \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_id \u001b[39m+\u001b[39;49m\n\u001b[1;32m   1715\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m proto\u001b[39m.\u001b[39;49mEND_COMMAND_PART)\n\u001b[1;32m   1716\u001b[0m \u001b[39mif\u001b[39;00m answer \u001b[39m==\u001b[39m proto\u001b[39m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[39mreturn\u001b[39;00m JavaPackage(name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client, jvm_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_id)\n",
      "File \u001b[0;32m~/Desktop/UNI/project-1-individual-umabarnes/.venv1/lib/python3.12/site-packages/py4j/java_gateway.py:1053\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(retry, connection, pne):\n\u001b[1;32m   1052\u001b[0m     logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mException while sending command.\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m-> 1053\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend_command(command, binary\u001b[39m=\u001b[39;49mbinary)\n\u001b[1;32m   1054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1055\u001b[0m     logging\u001b[39m.\u001b[39mexception(\n\u001b[1;32m   1056\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mException while sending command.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/UNI/project-1-individual-umabarnes/.venv1/lib/python3.12/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend_command\u001b[39m(\u001b[39mself\u001b[39m, command, retry\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[39m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[39m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[39m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_connection()\n\u001b[1;32m   1037\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/Desktop/UNI/project-1-individual-umabarnes/.venv1/lib/python3.12/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m connection\u001b[39m.\u001b[39msocket \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_new_connection()\n\u001b[1;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/Desktop/UNI/project-1-individual-umabarnes/.venv1/lib/python3.12/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_new_connection\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[39m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_parameters, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_property, \u001b[39mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     connection\u001b[39m.\u001b[39;49mconnect_to_java_server()\n\u001b[1;32m    292\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/Desktop/UNI/project-1-individual-umabarnes/.venv1/lib/python3.12/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context\u001b[39m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket, server_hostname\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket\u001b[39m.\u001b[39;49mconnect((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_address, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_port))\n\u001b[1;32m    439\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket\u001b[39m.\u001b[39mmakefile(\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_connected \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Modeling\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------+-----+---+----+----+------+-------------+----------+-----------+--------+\n",
      "|pickup_date|pickup_hour|pulocationid|count|wnd| tmp| dew|   atm|service_count|is_weekday|day_of_week| borough|\n",
      "+-----------+-----------+------------+-----+---+----+----+------+-------------+----------+-----------+--------+\n",
      "| 2023-11-13|         15|          26|  224|3.1| 6.7|-3.3|1027.2|           14|      true|          2|Brooklyn|\n",
      "| 2023-11-12|          4|          29|   14|2.1| 5.6|-1.7|1030.9|           11|      true|          1|Brooklyn|\n",
      "| 2023-11-09|          0|          29|   14|2.1| 7.8|-4.4|1018.9|           29|      true|          5|Brooklyn|\n",
      "| 2023-11-12|          0|          29|   43|2.1| 7.8|-2.2|1029.7|            6|      true|          1|Brooklyn|\n",
      "| 2023-11-10|         13|          29|   56|2.1|10.6| 0.0|1020.2|           12|     false|          6|Brooklyn|\n",
      "+-----------+-----------+------------+-----+---+----+----+------+-------------+----------+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to the CSV file\n",
    "csv_file_path = \"../data/curated/merged.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/25 01:14:28 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: Queens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/25 01:14:32 WARN Instrumentation: [1281504b] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/25 01:14:33 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/08/25 01:14:33 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "24/08/25 01:14:33 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "24/08/25 01:14:35 WARN Instrumentation: [9b0d592c] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: EWR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/25 01:14:35 WARN Instrumentation: [9b0d592c] The standard deviation of the label is zero, so the coefficients will be zeros and the intercept will be the mean of the label; as a result, training is not needed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: Brooklyn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/25 01:14:36 WARN Instrumentation: [44124e42] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/25 01:14:37 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: Staten Island\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/25 01:14:37 WARN Instrumentation: [f882d804] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: Manhattan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/25 01:14:39 WARN Instrumentation: [8a68833b] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: Bronx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/25 01:14:40 WARN Instrumentation: [228f98eb] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: None\n",
      "Error processing borough None: An error occurred while calling o872.fit.\n",
      ": java.lang.AssertionError: assertion failed: Training dataset is empty.\n",
      "\tat scala.Predef$.assert(Predef.scala:223)\n",
      "\tat org.apache.spark.ml.optim.WeightedLeastSquares$Aggregator.validate(WeightedLeastSquares.scala:425)\n",
      "\tat org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:108)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.trainWithNormal(LinearRegression.scala:456)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:354)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:329)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:186)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Borough: Queens, RMSE: 113.92764592270076\n",
      "Borough: EWR, RMSE: 0.0\n",
      "Borough: Brooklyn, RMSE: 104.64477949410787\n",
      "Borough: Staten Island, RMSE: 13.209105016264825\n",
      "Borough: Manhattan, RMSE: 126.1565826899772\n",
      "Borough: Bronx, RMSE: 45.010672188966765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/25 01:14:41 WARN Instrumentation: [a106ac16] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/25 01:14:41 ERROR Instrumentation: java.lang.AssertionError: assertion failed: Training dataset is empty.\n",
      "\tat scala.Predef$.assert(Predef.scala:223)\n",
      "\tat org.apache.spark.ml.optim.WeightedLeastSquares$Aggregator.validate(WeightedLeastSquares.scala:425)\n",
      "\tat org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:108)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.trainWithNormal(LinearRegression.scala:456)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:354)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:329)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:186)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"TaxiDemandForecasting\").getOrCreate()\n",
    "\n",
    "\n",
    "# Convert categorical column to numerical if needed\n",
    "if 'pulocationid_indexed' not in df.columns:\n",
    "    indexer = StringIndexer(inputCol=\"pulocationid\", outputCol=\"pulocationid_indexed\")\n",
    "    df = indexer.fit(df).transform(df)\n",
    "\n",
    "# Define feature columns\n",
    "feature_columns = [\"pickup_hour\", \"pulocationid_indexed\", \"wnd\", \"tmp\", \"dew\", \"atm\", \"service_count\"]\n",
    "\n",
    "# Assemble features into a vector\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Rename 'count' column to 'label'\n",
    "df = df.withColumnRenamed(\"count\", \"label\")\n",
    "\n",
    "# List of columns to check for missing values\n",
    "columns_to_check = [\"pickup_hour\", \"pulocationid\", \"wnd\", \"tmp\", \"dew\", \"atm\", \"service_count\", \"label\"]\n",
    "\n",
    "# Check for missing values in relevant columns\n",
    "for column in columns_to_check:\n",
    "    if column in df.columns:\n",
    "        missing_count = df.filter(col(column).isNull()).count()\n",
    "        if missing_count > 0:\n",
    "            print(f\"Column {column} has {missing_count} missing values.\")\n",
    "    else:\n",
    "        print(f\"Column {column} does not exist in the DataFrame.\")\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "# Perform linear regression for each borough\n",
    "for borough in df.select(\"borough\").distinct().rdd.flatMap(lambda x: x).collect():\n",
    "    print(f\"Processing borough: {borough}\")\n",
    "    \n",
    "    # Filter DataFrame for the current borough\n",
    "    df_borough = df.filter(df.borough == borough)\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    train_df, test_df = df_borough.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    # Initialize the Linear Regression model\n",
    "    lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "    \n",
    "    # Train the model\n",
    "    try:\n",
    "        lr_model = lr.fit(train_df)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = lr_model.transform(test_df)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "        \n",
    "        # Store results\n",
    "        results.append((borough, rmse))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing borough {borough}: {e}\")\n",
    "\n",
    "# Print results\n",
    "for borough, rmse in results:\n",
    "    print(f\"Borough: {borough}, RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/25 01:14:41 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: Queens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/25 01:14:43 WARN Instrumentation: [7e3e3096] regParam is zero, which might cause numerical instability and overfitting.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: EWR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/25 01:14:45 WARN Instrumentation: [82e37b64] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/25 01:14:45 WARN Instrumentation: [82e37b64] The standard deviation of the label is zero, so the coefficients will be zeros and the intercept will be the mean of the label; as a result, training is not needed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: Brooklyn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/25 01:14:46 WARN Instrumentation: [6f58707a] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: Staten Island\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/25 01:14:47 WARN Instrumentation: [12b60553] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: Manhattan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/25 01:14:48 WARN Instrumentation: [1726d1ad] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: Bronx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/25 01:14:50 WARN Instrumentation: [8aebf045] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: None\n",
      "Error processing borough None: An error occurred while calling o1774.fit.\n",
      ": java.lang.AssertionError: assertion failed: Training dataset is empty.\n",
      "\tat scala.Predef$.assert(Predef.scala:223)\n",
      "\tat org.apache.spark.ml.optim.WeightedLeastSquares$Aggregator.validate(WeightedLeastSquares.scala:425)\n",
      "\tat org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:108)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.trainWithNormal(LinearRegression.scala:456)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:354)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:329)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:186)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/25 01:14:51 WARN Instrumentation: [b70c5e9a] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/25 01:14:51 ERROR Instrumentation: java.lang.AssertionError: assertion failed: Training dataset is empty.\n",
      "\tat scala.Predef$.assert(Predef.scala:223)\n",
      "\tat org.apache.spark.ml.optim.WeightedLeastSquares$Aggregator.validate(WeightedLeastSquares.scala:425)\n",
      "\tat org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:108)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.trainWithNormal(LinearRegression.scala:456)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:354)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:329)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:186)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------------------+-------+\n",
      "|pickup_date|actual|         predicted|borough|\n",
      "+-----------+------+------------------+-------+\n",
      "| 2023-08-01|    23| 42.22920369072207| Queens|\n",
      "| 2023-08-01|    25|  66.3013031269088| Queens|\n",
      "| 2023-08-01|    33| 62.21764340112712| Queens|\n",
      "| 2023-08-01|    37| 43.73370990548375| Queens|\n",
      "| 2023-08-01|    20| 66.11517065502188| Queens|\n",
      "| 2023-08-01|    21|  58.8075690404652| Queens|\n",
      "| 2023-08-01|    58|  44.2088110489309| Queens|\n",
      "| 2023-08-01|    50| 75.58851209967432| Queens|\n",
      "| 2023-08-01|   102|47.547367060276294| Queens|\n",
      "| 2023-08-01|    36|  83.0107278368014| Queens|\n",
      "| 2023-08-01|     5|37.230753016196275| Queens|\n",
      "| 2023-08-01|    38| 66.46115947442301| Queens|\n",
      "| 2023-08-01|    10|30.997798697897917| Queens|\n",
      "| 2023-08-01|    35| 50.72912793566094| Queens|\n",
      "| 2023-08-01|    30| 72.22207386082766| Queens|\n",
      "| 2023-08-01|    56| 89.36770795907557| Queens|\n",
      "| 2023-08-01|    53| 92.03792317320514| Queens|\n",
      "| 2023-08-01|    90| 99.73818781792677| Queens|\n",
      "| 2023-08-01|    24| 63.53742206806604| Queens|\n",
      "| 2023-08-01|   172| 99.43064176309447| Queens|\n",
      "+-----------+------+------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Borough: Queens, RMSE: 113.92764592270076\n",
      "Borough: EWR, RMSE: 0.0\n",
      "Borough: Brooklyn, RMSE: 104.64477949410787\n",
      "Borough: Staten Island, RMSE: 13.209105016264825\n",
      "Borough: Manhattan, RMSE: 126.1565826899772\n",
      "Borough: Bronx, RMSE: 45.010672188966765\n",
      "Borough: Queens, RMSE: 113.92764592270076\n",
      "Borough: EWR, RMSE: 0.0\n",
      "Borough: Brooklyn, RMSE: 104.64477949410784\n",
      "Borough: Staten Island, RMSE: 13.209105016264822\n",
      "Borough: Manhattan, RMSE: 126.1565826899772\n",
      "Borough: Bronx, RMSE: 45.010672188966765\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"RideshareDemandForecasting\").getOrCreate()\n",
    "\n",
    "# Assume df is already initialized\n",
    "# Define the new DataFrame as lr_results\n",
    "lr_results = df\n",
    "\n",
    "# Convert categorical column to numerical if needed\n",
    "if 'pulocationid_indexed' not in lr_results.columns:\n",
    "    indexer = StringIndexer(inputCol=\"pulocationid\", outputCol=\"pulocationid_indexed\")\n",
    "    lr_results = indexer.fit(lr_results).transform(lr_results)\n",
    "\n",
    "# Define feature columns\n",
    "feature_columns = [\"pickup_hour\", \"pulocationid_indexed\", \"wnd\", \"tmp\", \"dew\", \"atm\", \"service_count\"]\n",
    "\n",
    "# Check if 'features' column already exists and drop it if necessary\n",
    "if 'features' in lr_results.columns:\n",
    "    lr_results = lr_results.drop(\"features\")\n",
    "\n",
    "# Assemble features into a vector\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "lr_results = assembler.transform(lr_results)\n",
    "\n",
    "# Rename 'count' column to 'label'\n",
    "lr_results = lr_results.withColumnRenamed(\"count\", \"label\")\n",
    "\n",
    "# List of columns to check for missing values\n",
    "columns_to_check = [\"pickup_hour\", \"pulocationid\", \"wnd\", \"tmp\", \"dew\", \"atm\", \"service_count\", \"label\"]\n",
    "\n",
    "# Check for missing values in relevant columns\n",
    "for column in columns_to_check:\n",
    "    if column in lr_results.columns:\n",
    "        missing_count = lr_results.filter(col(column).isNull()).count()\n",
    "        if missing_count > 0:\n",
    "            print(f\"Column {column} has {missing_count} missing values.\")\n",
    "    else:\n",
    "        print(f\"Column {column} does not exist in the DataFrame.\")\n",
    "\n",
    "# List to store DataFrames with actual and predicted values\n",
    "predictions_list = []\n",
    "\n",
    "# Perform linear regression for each borough\n",
    "for borough in lr_results.select(\"borough\").distinct().rdd.flatMap(lambda x: x).collect():\n",
    "    print(f\"Processing borough: {borough}\")\n",
    "    \n",
    "    # Filter DataFrame for the current borough\n",
    "    df_borough = lr_results.filter(lr_results.borough == borough)\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    train_df, test_df = df_borough.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    # Initialize the Linear Regression model\n",
    "    lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "    \n",
    "    # Train the model\n",
    "    try:\n",
    "        lr_model = lr.fit(train_df)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = lr_model.transform(test_df)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "        \n",
    "        # Collect actual and predicted values\n",
    "        predictions_df = predictions.select(col(\"pickup_date\"), col(\"label\").alias(\"actual\"), col(\"prediction\").alias(\"predicted\"))\n",
    "        \n",
    "        # Add a column for the borough\n",
    "        predictions_df = predictions_df.withColumn(\"borough\", lit(borough))\n",
    "        \n",
    "        # Append to the list\n",
    "        predictions_list.append(predictions_df)\n",
    "        \n",
    "        # Store RMSE results\n",
    "        results.append((borough, rmse))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing borough {borough}: {e}\")\n",
    "\n",
    "# Combine all the DataFrames in the list into one DataFrame\n",
    "if predictions_list:\n",
    "    lr_results = predictions_list[0]\n",
    "    for df in predictions_list[1:]:\n",
    "        lr_results = lr_results.union(df)\n",
    "\n",
    "    # Show the combined DataFrame\n",
    "    lr_results.show()\n",
    "else:\n",
    "    print(\"No predictions were made.\")\n",
    "\n",
    "# Print RMSE results\n",
    "for borough, rmse in results:\n",
    "    print(f\"Borough: {borough}, RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------+-----+---+----+----+------+-------------+----------+-----------+--------+\n",
      "|pickup_date|pickup_hour|pulocationid|count|wnd| tmp| dew|   atm|service_count|is_weekday|day_of_week| borough|\n",
      "+-----------+-----------+------------+-----+---+----+----+------+-------------+----------+-----------+--------+\n",
      "| 2023-11-13|         15|          26|  224|3.1| 6.7|-3.3|1027.2|           14|      true|          2|Brooklyn|\n",
      "| 2023-11-12|          4|          29|   14|2.1| 5.6|-1.7|1030.9|           11|      true|          1|Brooklyn|\n",
      "| 2023-11-09|          0|          29|   14|2.1| 7.8|-4.4|1018.9|           29|      true|          5|Brooklyn|\n",
      "| 2023-11-12|          0|          29|   43|2.1| 7.8|-2.2|1029.7|            6|      true|          1|Brooklyn|\n",
      "| 2023-11-10|         13|          29|   56|2.1|10.6| 0.0|1020.2|           12|     false|          6|Brooklyn|\n",
      "+-----------+-----------+------------+-----+---+----+----+------+-------------+----------+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to the CSV file\n",
    "csv_file_path = \"../data/curated/merged.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pickup_date: date (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- pulocationid: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- wnd: double (nullable = true)\n",
      " |-- tmp: double (nullable = true)\n",
      " |-- dew: double (nullable = true)\n",
      " |-- atm: double (nullable = true)\n",
      " |-- service_count: integer (nullable = true)\n",
      " |-- is_weekday: boolean (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- borough: string (nullable = true)\n",
      " |-- pulocationid_indexed: double (nullable = false)\n",
      "\n",
      "+------------+--------------------+\n",
      "|pulocationid|pulocationid_indexed|\n",
      "+------------+--------------------+\n",
      "|          26|               131.0|\n",
      "|          29|               137.0|\n",
      "|          29|               137.0|\n",
      "|          29|               137.0|\n",
      "|          29|               137.0|\n",
      "+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Create `pulocationid_indexed` if it does not exist\n",
    "if 'pulocationid_indexed' not in df.columns:\n",
    "    indexer = StringIndexer(inputCol=\"pulocationid\", outputCol=\"pulocationid_indexed\")\n",
    "    df = indexer.fit(df).transform(df)\n",
    "\n",
    "# Verify the new column\n",
    "df.printSchema()\n",
    "df.select(\"pulocationid\", \"pulocationid_indexed\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pickup_date: date (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- pulocationid: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- wnd: double (nullable = true)\n",
      " |-- tmp: double (nullable = true)\n",
      " |-- dew: double (nullable = true)\n",
      " |-- atm: double (nullable = true)\n",
      " |-- service_count: integer (nullable = true)\n",
      " |-- is_weekday: boolean (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- borough: string (nullable = true)\n",
      " |-- pulocationid_indexed: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+-------------------------------------+\n",
      "|features                             |\n",
      "+-------------------------------------+\n",
      "|[15.0,131.0,3.1,6.7,-3.3,1027.2,14.0]|\n",
      "|[4.0,137.0,2.1,5.6,-1.7,1030.9,11.0] |\n",
      "|[0.0,137.0,2.1,7.8,-4.4,1018.9,29.0] |\n",
      "|[0.0,137.0,2.1,7.8,-2.2,1029.7,6.0]  |\n",
      "|[13.0,137.0,2.1,10.6,0.0,1020.2,12.0]|\n",
      "+-------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Define the feature columns\n",
    "feature_columns = [\"pickup_hour\", \"pulocationid_indexed\", \"wnd\", \"tmp\", \"dew\", \"atm\", \"service_count\"]\n",
    "\n",
    "# Create the VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Transform the DataFrame to include the 'features' column\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Verify the addition of the 'features' column\n",
    "df.printSchema()\n",
    "df.select(\"features\").show(5, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: Queens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: EWR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/25 01:14:58 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 500 to 3 (= number of training instances)\n",
      "24/08/25 01:14:58 ERROR Instrumentation: java.lang.IllegalArgumentException: requirement failed: DecisionTree requires maxBins (= 3) to be at least as large as the number of values in each categorical feature, but categorical feature 1 has 261 values. Consider removing this and other categorical features with a large number of values, or add more training examples.\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:151)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:158)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:136)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:45)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing borough EWR: requirement failed: DecisionTree requires maxBins (= 3) to be at least as large as the number of values in each categorical feature, but categorical feature 1 has 261 values. Consider removing this and other categorical features with a large number of values, or add more training examples.\n",
      "Processing borough: Brooklyn\n",
      "Processing borough: Staten Island\n",
      "Processing borough: Manhattan\n",
      "Processing borough: Bronx\n",
      "Processing borough: None\n",
      "Skipping borough None: DataFrame is empty.\n",
      "+-----------+------+------------------+-------+\n",
      "|pickup_date|actual|         predicted|borough|\n",
      "+-----------+------+------------------+-------+\n",
      "| 2023-08-01|    23| 31.68951768031703| Queens|\n",
      "| 2023-08-01|    25|  32.4597477406004| Queens|\n",
      "| 2023-08-01|    33| 74.24533503861026| Queens|\n",
      "| 2023-08-01|    37| 47.78658670143301| Queens|\n",
      "| 2023-08-01|    20| 57.83918577437648| Queens|\n",
      "| 2023-08-01|    21| 74.77615237059013| Queens|\n",
      "| 2023-08-01|    58| 138.4367424147328| Queens|\n",
      "| 2023-08-01|    50|110.20416343074221| Queens|\n",
      "| 2023-08-01|   102|141.43352611002743| Queens|\n",
      "| 2023-08-01|    36| 40.50402601298818| Queens|\n",
      "| 2023-08-01|     5|26.769334261973682| Queens|\n",
      "| 2023-08-01|    38|109.07812386655687| Queens|\n",
      "| 2023-08-01|    10|26.491003398702723| Queens|\n",
      "| 2023-08-01|    35|  35.4428570293233| Queens|\n",
      "| 2023-08-01|    30| 33.48451993700885| Queens|\n",
      "| 2023-08-01|    56| 55.61908833111403| Queens|\n",
      "| 2023-08-01|    53| 46.02213879797748| Queens|\n",
      "| 2023-08-01|    90| 66.49839159224825| Queens|\n",
      "| 2023-08-01|    24| 47.25681971247148| Queens|\n",
      "| 2023-08-01|   172|181.43400767366967| Queens|\n",
      "+-----------+------+------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Borough: Queens, RMSE: 52.94356016011966\n",
      "Borough: Brooklyn, RMSE: 64.24999907436175\n",
      "Borough: Staten Island, RMSE: 9.282105578973452\n",
      "Borough: Manhattan, RMSE: 83.1682327412733\n",
      "Borough: Bronx, RMSE: 28.512321031403683\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# Rename the DataFrame to rf_df\n",
    "rf_df = df\n",
    "\n",
    "# Rename 'count' column to 'label'\n",
    "rf_df = rf_df.withColumnRenamed(\"count\", \"label\")\n",
    "\n",
    "# List to store results\n",
    "results_rf = []\n",
    "\n",
    "# List to store DataFrames with actual and predicted values\n",
    "predictions_list_rf = []\n",
    "\n",
    "# Perform Random Forest Regression for each borough\n",
    "for borough in rf_df.select(\"borough\").distinct().rdd.flatMap(lambda x: x).collect():\n",
    "    print(f\"Processing borough: {borough}\")\n",
    "\n",
    "    # Filter DataFrame for the current borough\n",
    "    df_borough = rf_df.filter(rf_df.borough == borough)\n",
    "\n",
    "    # Check if 'features' and 'label' columns exist\n",
    "    if 'features' not in df_borough.columns or 'label' not in df_borough.columns:\n",
    "        print(f\"Skipping borough {borough}: required columns are missing.\")\n",
    "        continue\n",
    "\n",
    "    # Check if DataFrame is empty\n",
    "    if df_borough.count() == 0:\n",
    "        print(f\"Skipping borough {borough}: DataFrame is empty.\")\n",
    "        continue\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    train_df, test_df = df_borough.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    # Initialize the Random Forest model with increased maxBins\n",
    "    rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\", maxBins=500)\n",
    "\n",
    "    # Train the model\n",
    "    try:\n",
    "        rf_model = rf.fit(train_df)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = rf_model.transform(test_df)\n",
    "\n",
    "        # Evaluate the model\n",
    "        evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "        # Collect actual and predicted values\n",
    "        predictions_df = predictions.select(col(\"pickup_date\"), col(\"label\").alias(\"actual\"), col(\"prediction\").alias(\"predicted\"))\n",
    "        \n",
    "        # Add a column for the borough\n",
    "        predictions_df = predictions_df.withColumn(\"borough\", lit(borough))\n",
    "        \n",
    "        # Append to the list\n",
    "        predictions_list_rf.append(predictions_df)\n",
    "        \n",
    "        # Store RMSE results\n",
    "        results_rf.append((borough, rmse))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing borough {borough}: {e}\")\n",
    "\n",
    "# Combine all the DataFrames in the list into one DataFrame\n",
    "if predictions_list_rf:\n",
    "    rf_results = predictions_list_rf[0]\n",
    "    for df in predictions_list_rf[1:]:\n",
    "        rf_results = rf_results.union(df)\n",
    "\n",
    "    # Show the combined DataFrame\n",
    "    rf_results.show()\n",
    "else:\n",
    "    print(\"No predictions were made.\")\n",
    "\n",
    "# Print RMSE results\n",
    "for borough, rmse in results_rf:\n",
    "    print(f\"Borough: {borough}, RMSE: {rmse}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.4 ('.venv1': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc19f976deae78a4b8dd1c0db37710e451de00dcd2e448b10d5869a2fb538e25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
