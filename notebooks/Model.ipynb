{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 14:31:16 WARN Utils: Your hostname, MacBook-Pro-7.local resolves to a loopback address: 127.0.0.1; using 10.95.1.206 instead (on interface en0)\n",
      "24/08/26 14:31:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/26 14:31:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/08/26 14:31:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/08/26 14:31:17 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/08/26 14:31:17 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "24/08/26 14:31:17 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "24/08/26 14:31:17 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "24/08/26 14:31:17 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Modeling\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------+-----+---+----+----+------+-------------+----------+-----------+--------+\n",
      "|pickup_date|pickup_hour|pulocationid|count|wnd| tmp| dew|   atm|service_count|is_weekday|day_of_week| borough|\n",
      "+-----------+-----------+------------+-----+---+----+----+------+-------------+----------+-----------+--------+\n",
      "| 2023-11-13|         15|          26|  224|3.1| 6.7|-3.3|1027.2|           14|      true|          2|Brooklyn|\n",
      "| 2023-11-12|          4|          29|   14|2.1| 5.6|-1.7|1030.9|           11|      true|          1|Brooklyn|\n",
      "| 2023-11-09|          0|          29|   14|2.1| 7.8|-4.4|1018.9|           29|      true|          5|Brooklyn|\n",
      "| 2023-11-12|          0|          29|   43|2.1| 7.8|-2.2|1029.7|            6|      true|          1|Brooklyn|\n",
      "| 2023-11-10|         13|          29|   56|2.1|10.6| 0.0|1020.2|           12|     false|          6|Brooklyn|\n",
      "+-----------+-----------+------------+-----+---+----+----+------+-------------+----------+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to the CSV file\n",
    "csv_file_path = \"../data/curated/merged.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 14:31:25 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "24/08/26 14:31:28 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: Queens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 14:31:29 WARN Instrumentation: [69d2236b] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/26 14:31:30 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/08/26 14:31:30 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "24/08/26 14:31:31 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: EWR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 14:31:33 WARN Instrumentation: [98ac58ce] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/26 14:31:33 WARN Instrumentation: [98ac58ce] The standard deviation of the label is zero, so the coefficients will be zeros and the intercept will be the mean of the label; as a result, training is not needed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: Brooklyn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 14:31:34 WARN Instrumentation: [a7fba458] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: Staten Island\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 14:31:36 WARN Instrumentation: [7b2730af] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: Manhattan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 14:31:38 WARN Instrumentation: [f654f5cc] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: Bronx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 14:31:39 WARN Instrumentation: [3e8502e7] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: None\n",
      "Error processing borough None: An error occurred while calling o872.fit.\n",
      ": java.lang.AssertionError: assertion failed: Training dataset is empty.\n",
      "\tat scala.Predef$.assert(Predef.scala:223)\n",
      "\tat org.apache.spark.ml.optim.WeightedLeastSquares$Aggregator.validate(WeightedLeastSquares.scala:425)\n",
      "\tat org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:108)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.trainWithNormal(LinearRegression.scala:456)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:354)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:329)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:186)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Borough: Queens, RMSE: 113.92764592270076\n",
      "Borough: EWR, RMSE: 0.0\n",
      "Borough: Brooklyn, RMSE: 104.64477949410787\n",
      "Borough: Staten Island, RMSE: 13.209105016264822\n",
      "Borough: Manhattan, RMSE: 126.1565826899772\n",
      "Borough: Bronx, RMSE: 45.010672188966765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 14:31:40 WARN Instrumentation: [be083044] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/26 14:31:40 ERROR Instrumentation: java.lang.AssertionError: assertion failed: Training dataset is empty.\n",
      "\tat scala.Predef$.assert(Predef.scala:223)\n",
      "\tat org.apache.spark.ml.optim.WeightedLeastSquares$Aggregator.validate(WeightedLeastSquares.scala:425)\n",
      "\tat org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:108)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.trainWithNormal(LinearRegression.scala:456)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:354)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:329)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:186)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convert categorical column to numerical if needed\n",
    "if 'pulocationid_indexed' not in df.columns:\n",
    "    indexer = StringIndexer(inputCol=\"pulocationid\", outputCol=\"pulocationid_indexed\")\n",
    "    df = indexer.fit(df).transform(df)\n",
    "\n",
    "# Define feature columns\n",
    "feature_columns = [\"pickup_hour\", \"pulocationid_indexed\", \"wnd\", \"tmp\", \"dew\", \"atm\", \"service_count\"]\n",
    "\n",
    "# Assemble features into a vector\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Rename 'count' column to 'label'\n",
    "df = df.withColumnRenamed(\"count\", \"label\")\n",
    "\n",
    "# List of columns to check for missing values\n",
    "columns_to_check = [\"pickup_hour\", \"pulocationid\", \"wnd\", \"tmp\", \"dew\", \"atm\", \"service_count\", \"label\"]\n",
    "\n",
    "# Check for missing values in relevant columns\n",
    "for column in columns_to_check:\n",
    "    if column in df.columns:\n",
    "        missing_count = df.filter(col(column).isNull()).count()\n",
    "        if missing_count > 0:\n",
    "            print(f\"Column {column} has {missing_count} missing values.\")\n",
    "    else:\n",
    "        print(f\"Column {column} does not exist in the DataFrame.\")\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "# Perform linear regression for each borough\n",
    "for borough in df.select(\"borough\").distinct().rdd.flatMap(lambda x: x).collect():\n",
    "    print(f\"Processing borough: {borough}\")\n",
    "    \n",
    "    # Filter DataFrame for the current borough\n",
    "    df_borough = df.filter(df.borough == borough)\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    train_df, test_df = df_borough.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    # Initialize the Linear Regression model\n",
    "    lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "    \n",
    "    # Train the model\n",
    "    try:\n",
    "        lr_model = lr.fit(train_df)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = lr_model.transform(test_df)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "        \n",
    "        # Store results\n",
    "        results.append((borough, rmse))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing borough {borough}: {e}\")\n",
    "\n",
    "# Print results\n",
    "for borough, rmse in results:\n",
    "    print(f\"Borough: {borough}, RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: Queens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 14:32:08 WARN Instrumentation: [5c3199b9] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/26 14:32:10 WARN Instrumentation: [35a24622] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: EWR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 14:32:11 WARN Instrumentation: [35a24622] The standard deviation of the label is zero, so the coefficients will be zeros and the intercept will be the mean of the label; as a result, training is not needed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: Brooklyn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 14:32:12 WARN Instrumentation: [aadcf887] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: Staten Island\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 14:32:13 WARN Instrumentation: [bdeeb8af] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: Manhattan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 14:32:14 WARN Instrumentation: [945cd7a3] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: Bronx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 14:32:16 WARN Instrumentation: [5a2f0a69] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: None\n",
      "Error processing borough None: An error occurred while calling o1772.fit.\n",
      ": java.lang.AssertionError: assertion failed: Training dataset is empty.\n",
      "\tat scala.Predef$.assert(Predef.scala:223)\n",
      "\tat org.apache.spark.ml.optim.WeightedLeastSquares$Aggregator.validate(WeightedLeastSquares.scala:425)\n",
      "\tat org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:108)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.trainWithNormal(LinearRegression.scala:456)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:354)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:329)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:186)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 14:32:17 WARN Instrumentation: [a17fd765] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/26 14:32:17 ERROR Instrumentation: java.lang.AssertionError: assertion failed: Training dataset is empty.\n",
      "\tat scala.Predef$.assert(Predef.scala:223)\n",
      "\tat org.apache.spark.ml.optim.WeightedLeastSquares$Aggregator.validate(WeightedLeastSquares.scala:425)\n",
      "\tat org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:108)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.trainWithNormal(LinearRegression.scala:456)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:354)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:329)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:186)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------------------+-------+\n",
      "|pickup_date|actual|         predicted|borough|\n",
      "+-----------+------+------------------+-------+\n",
      "| 2023-08-01|    23| 42.22920369072207| Queens|\n",
      "| 2023-08-01|    25|  66.3013031269088| Queens|\n",
      "| 2023-08-01|    33| 62.21764340112712| Queens|\n",
      "| 2023-08-01|    37| 43.73370990548375| Queens|\n",
      "| 2023-08-01|    20| 66.11517065502188| Queens|\n",
      "| 2023-08-01|    21|  58.8075690404652| Queens|\n",
      "| 2023-08-01|    58|  44.2088110489309| Queens|\n",
      "| 2023-08-01|    50| 75.58851209967432| Queens|\n",
      "| 2023-08-01|   102|47.547367060276294| Queens|\n",
      "| 2023-08-01|    36|  83.0107278368014| Queens|\n",
      "| 2023-08-01|     5|37.230753016196275| Queens|\n",
      "| 2023-08-01|    38| 66.46115947442301| Queens|\n",
      "| 2023-08-01|    10|30.997798697897917| Queens|\n",
      "| 2023-08-01|    35| 50.72912793566094| Queens|\n",
      "| 2023-08-01|    30| 72.22207386082766| Queens|\n",
      "| 2023-08-01|    56| 89.36770795907557| Queens|\n",
      "| 2023-08-01|    53| 92.03792317320514| Queens|\n",
      "| 2023-08-01|    90| 99.73818781792677| Queens|\n",
      "| 2023-08-01|    24| 63.53742206806604| Queens|\n",
      "| 2023-08-01|   172| 99.43064176309447| Queens|\n",
      "+-----------+------+------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Borough: Queens, RMSE: 113.92764592270076\n",
      "Borough: EWR, RMSE: 0.0\n",
      "Borough: Brooklyn, RMSE: 104.64477949410787\n",
      "Borough: Staten Island, RMSE: 13.209105016264822\n",
      "Borough: Manhattan, RMSE: 126.1565826899772\n",
      "Borough: Bronx, RMSE: 45.010672188966765\n",
      "Borough: Queens, RMSE: 113.92764592270076\n",
      "Borough: EWR, RMSE: 0.0\n",
      "Borough: Brooklyn, RMSE: 104.64477949410785\n",
      "Borough: Staten Island, RMSE: 13.209105016264825\n",
      "Borough: Manhattan, RMSE: 126.1565826899772\n",
      "Borough: Bronx, RMSE: 45.010672188966765\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "\n",
    "# Assume df is already initialized\n",
    "# Define the new DataFrame as lr_results\n",
    "lr_results = df\n",
    "\n",
    "# Convert categorical column to numerical if needed\n",
    "if 'pulocationid_indexed' not in lr_results.columns:\n",
    "    indexer = StringIndexer(inputCol=\"pulocationid\", outputCol=\"pulocationid_indexed\")\n",
    "    lr_results = indexer.fit(lr_results).transform(lr_results)\n",
    "\n",
    "# Define feature columns\n",
    "feature_columns = [\"pickup_hour\", \"pulocationid_indexed\", \"wnd\", \"tmp\", \"dew\", \"atm\", \"service_count\"]\n",
    "\n",
    "# Check if 'features' column already exists and drop it if necessary\n",
    "if 'features' in lr_results.columns:\n",
    "    lr_results = lr_results.drop(\"features\")\n",
    "\n",
    "# Assemble features into a vector\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "lr_results = assembler.transform(lr_results)\n",
    "\n",
    "# Rename 'count' column to 'label'\n",
    "lr_results = lr_results.withColumnRenamed(\"count\", \"label\")\n",
    "\n",
    "# List of columns to check for missing values\n",
    "columns_to_check = [\"pickup_hour\", \"pulocationid\", \"wnd\", \"tmp\", \"dew\", \"atm\", \"service_count\", \"label\"]\n",
    "\n",
    "# Check for missing values in relevant columns\n",
    "for column in columns_to_check:\n",
    "    if column in lr_results.columns:\n",
    "        missing_count = lr_results.filter(col(column).isNull()).count()\n",
    "        if missing_count > 0:\n",
    "            print(f\"Column {column} has {missing_count} missing values.\")\n",
    "    else:\n",
    "        print(f\"Column {column} does not exist in the DataFrame.\")\n",
    "\n",
    "# List to store DataFrames with actual and predicted values\n",
    "predictions_list = []\n",
    "\n",
    "# Perform linear regression for each borough\n",
    "for borough in lr_results.select(\"borough\").distinct().rdd.flatMap(lambda x: x).collect():\n",
    "    print(f\"Processing borough: {borough}\")\n",
    "    \n",
    "    # Filter DataFrame for the current borough\n",
    "    df_borough = lr_results.filter(lr_results.borough == borough)\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    train_df, test_df = df_borough.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    # Initialize the Linear Regression model\n",
    "    lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "    \n",
    "    # Train the model\n",
    "    try:\n",
    "        lr_model = lr.fit(train_df)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = lr_model.transform(test_df)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "        \n",
    "        # Collect actual and predicted values\n",
    "        predictions_df = predictions.select(col(\"pickup_date\"), col(\"label\").alias(\"actual\"), col(\"prediction\").alias(\"predicted\"))\n",
    "        \n",
    "        # Add a column for the borough\n",
    "        predictions_df = predictions_df.withColumn(\"borough\", lit(borough))\n",
    "        \n",
    "        # Append to the list\n",
    "        predictions_list.append(predictions_df)\n",
    "        \n",
    "        # Store RMSE results\n",
    "        results.append((borough, rmse))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing borough {borough}: {e}\")\n",
    "\n",
    "# Combine all the DataFrames in the list into one DataFrame\n",
    "if predictions_list:\n",
    "    lr_results = predictions_list[0]\n",
    "    for df in predictions_list[1:]:\n",
    "        lr_results = lr_results.union(df)\n",
    "\n",
    "    # Show the combined DataFrame\n",
    "    lr_results.show()\n",
    "else:\n",
    "    print(\"No predictions were made.\")\n",
    "\n",
    "# Print RMSE results\n",
    "for borough, rmse in results:\n",
    "    print(f\"Borough: {borough}, RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [2.539705004182333,-0.3215634246083111,-0.06258068730401058,-0.625553071735154,0.32995540446423444,0.09363428064445994,-0.4618547604978454]\n",
      "Intercept: 0.9504362842460911\n",
      "Feature: pickup_hour, Coefficient: 2.539705004182333\n",
      "Feature: pulocationid_indexed, Coefficient: -0.3215634246083111\n",
      "Feature: wnd, Coefficient: -0.06258068730401058\n",
      "Feature: tmp, Coefficient: -0.625553071735154\n",
      "Feature: dew, Coefficient: 0.32995540446423444\n",
      "Feature: atm, Coefficient: 0.09363428064445994\n",
      "Feature: service_count, Coefficient: -0.4618547604978454\n"
     ]
    }
   ],
   "source": [
    "# Get the coefficients and intercept\n",
    "coefficients = lr_model.coefficients\n",
    "intercept = lr_model.intercept\n",
    "\n",
    "    # Print the coefficients and intercept\n",
    "print(f\"Coefficients: {coefficients}\")\n",
    "print(f\"Intercept: {intercept}\")\n",
    "\n",
    "    # You can also print the coefficients for individual features if needed\n",
    "feature_names = feature_columns\n",
    "for feature, coef in zip(feature_names, coefficients):\n",
    "    print(f\"Feature: {feature}, Coefficient: {coef}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------+-----+---+----+----+------+-------------+----------+-----------+--------+\n",
      "|pickup_date|pickup_hour|pulocationid|count|wnd| tmp| dew|   atm|service_count|is_weekday|day_of_week| borough|\n",
      "+-----------+-----------+------------+-----+---+----+----+------+-------------+----------+-----------+--------+\n",
      "| 2023-11-13|         15|          26|  224|3.1| 6.7|-3.3|1027.2|           14|      true|          2|Brooklyn|\n",
      "| 2023-11-12|          4|          29|   14|2.1| 5.6|-1.7|1030.9|           11|      true|          1|Brooklyn|\n",
      "| 2023-11-09|          0|          29|   14|2.1| 7.8|-4.4|1018.9|           29|      true|          5|Brooklyn|\n",
      "| 2023-11-12|          0|          29|   43|2.1| 7.8|-2.2|1029.7|            6|      true|          1|Brooklyn|\n",
      "| 2023-11-10|         13|          29|   56|2.1|10.6| 0.0|1020.2|           12|     false|          6|Brooklyn|\n",
      "+-----------+-----------+------------+-----+---+----+----+------+-------------+----------+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to the CSV file\n",
    "csv_file_path = \"../data/curated/merged.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pickup_date: date (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- pulocationid: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- wnd: double (nullable = true)\n",
      " |-- tmp: double (nullable = true)\n",
      " |-- dew: double (nullable = true)\n",
      " |-- atm: double (nullable = true)\n",
      " |-- service_count: integer (nullable = true)\n",
      " |-- is_weekday: boolean (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- borough: string (nullable = true)\n",
      " |-- pulocationid_indexed: double (nullable = false)\n",
      "\n",
      "+------------+--------------------+\n",
      "|pulocationid|pulocationid_indexed|\n",
      "+------------+--------------------+\n",
      "|          26|               131.0|\n",
      "|          29|               137.0|\n",
      "|          29|               137.0|\n",
      "|          29|               137.0|\n",
      "|          29|               137.0|\n",
      "+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Create `pulocationid_indexed` if it does not exist\n",
    "if 'pulocationid_indexed' not in df.columns:\n",
    "    indexer = StringIndexer(inputCol=\"pulocationid\", outputCol=\"pulocationid_indexed\")\n",
    "    df = indexer.fit(df).transform(df)\n",
    "\n",
    "# Verify the new column\n",
    "df.printSchema()\n",
    "df.select(\"pulocationid\", \"pulocationid_indexed\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pickup_date: date (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- pulocationid: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- wnd: double (nullable = true)\n",
      " |-- tmp: double (nullable = true)\n",
      " |-- dew: double (nullable = true)\n",
      " |-- atm: double (nullable = true)\n",
      " |-- service_count: integer (nullable = true)\n",
      " |-- is_weekday: boolean (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- borough: string (nullable = true)\n",
      " |-- pulocationid_indexed: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+-------------------------------------+\n",
      "|features                             |\n",
      "+-------------------------------------+\n",
      "|[15.0,131.0,3.1,6.7,-3.3,1027.2,14.0]|\n",
      "|[4.0,137.0,2.1,5.6,-1.7,1030.9,11.0] |\n",
      "|[0.0,137.0,2.1,7.8,-4.4,1018.9,29.0] |\n",
      "|[0.0,137.0,2.1,7.8,-2.2,1029.7,6.0]  |\n",
      "|[13.0,137.0,2.1,10.6,0.0,1020.2,12.0]|\n",
      "+-------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Define the feature columns\n",
    "feature_columns = [\"pickup_hour\", \"pulocationid_indexed\", \"wnd\", \"tmp\", \"dew\", \"atm\", \"service_count\"]\n",
    "\n",
    "# Create the VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Transform the DataFrame to include the 'features' column\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Verify the addition of the 'features' column\n",
    "df.printSchema()\n",
    "df.select(\"features\").show(5, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: Queens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: EWR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 14:33:40 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 500 to 3 (= number of training instances)\n",
      "24/08/26 14:33:40 ERROR Instrumentation: java.lang.IllegalArgumentException: requirement failed: DecisionTree requires maxBins (= 3) to be at least as large as the number of values in each categorical feature, but categorical feature 1 has 261 values. Consider removing this and other categorical features with a large number of values, or add more training examples.\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:151)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:158)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:136)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:45)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing borough EWR: requirement failed: DecisionTree requires maxBins (= 3) to be at least as large as the number of values in each categorical feature, but categorical feature 1 has 261 values. Consider removing this and other categorical features with a large number of values, or add more training examples.\n",
      "Processing borough: Brooklyn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing borough: Staten Island\n",
      "Processing borough: Manhattan\n",
      "Processing borough: Bronx\n",
      "Processing borough: None\n",
      "Skipping borough None: DataFrame is empty.\n",
      "+-----------+------+------------------+-------+\n",
      "|pickup_date|actual|         predicted|borough|\n",
      "+-----------+------+------------------+-------+\n",
      "| 2023-08-01|    23|  29.2140696770817| Queens|\n",
      "| 2023-08-01|    25|29.487197402552585| Queens|\n",
      "| 2023-08-01|    33|  64.4857960938257| Queens|\n",
      "| 2023-08-01|    37| 40.16704303762327| Queens|\n",
      "| 2023-08-01|    20| 55.59631751206391| Queens|\n",
      "| 2023-08-01|    21| 66.53754740973802| Queens|\n",
      "| 2023-08-01|    58|135.38097865829357| Queens|\n",
      "| 2023-08-01|    50|109.59361367533113| Queens|\n",
      "| 2023-08-01|   102|134.63425582230235| Queens|\n",
      "| 2023-08-01|    36|33.607346881547436| Queens|\n",
      "| 2023-08-01|     5|23.680657173408104| Queens|\n",
      "| 2023-08-01|    38|113.54279190135932| Queens|\n",
      "| 2023-08-01|    10|23.680657173408104| Queens|\n",
      "| 2023-08-01|    35|32.134984630498586| Queens|\n",
      "| 2023-08-01|    30|26.083188192282392| Queens|\n",
      "| 2023-08-01|    56|  52.1454578782312| Queens|\n",
      "| 2023-08-01|    53| 58.21603921616861| Queens|\n",
      "| 2023-08-01|    90| 66.78581229878814| Queens|\n",
      "| 2023-08-01|    24| 58.01377341066505| Queens|\n",
      "| 2023-08-01|   172|169.15917403762182| Queens|\n",
      "+-----------+------+------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Borough: Queens, RMSE: 53.46314941324046\n",
      "Borough: Brooklyn, RMSE: 64.54518560516415\n",
      "Borough: Staten Island, RMSE: 9.123484695052875\n",
      "Borough: Manhattan, RMSE: 83.41974692423501\n",
      "Borough: Bronx, RMSE: 28.46030556571606\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# Rename the DataFrame to rf_df\n",
    "rf_df = df\n",
    "\n",
    "# Rename 'count' column to 'label'\n",
    "rf_df = rf_df.withColumnRenamed(\"count\", \"label\")\n",
    "\n",
    "# List to store results\n",
    "results_rf = []\n",
    "\n",
    "# List to store DataFrames with actual and predicted values\n",
    "predictions_list_rf = []\n",
    "\n",
    "# Perform Random Forest Regression for each borough\n",
    "for borough in rf_df.select(\"borough\").distinct().rdd.flatMap(lambda x: x).collect():\n",
    "    print(f\"Processing borough: {borough}\")\n",
    "\n",
    "    # Filter DataFrame for the current borough\n",
    "    df_borough = rf_df.filter(rf_df.borough == borough)\n",
    "\n",
    "    # Check if 'features' and 'label' columns exist\n",
    "    if 'features' not in df_borough.columns or 'label' not in df_borough.columns:\n",
    "        print(f\"Skipping borough {borough}: required columns are missing.\")\n",
    "        continue\n",
    "\n",
    "    # Check if DataFrame is empty\n",
    "    if df_borough.count() == 0:\n",
    "        print(f\"Skipping borough {borough}: DataFrame is empty.\")\n",
    "        continue\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    train_df, test_df = df_borough.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    # Initialize the Random Forest model with increased maxBins\n",
    "    rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\", maxBins=500)\n",
    "\n",
    "    # Train the model\n",
    "    try:\n",
    "        rf_model = rf.fit(train_df)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = rf_model.transform(test_df)\n",
    "\n",
    "        # Evaluate the model\n",
    "        evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "        # Collect actual and predicted values\n",
    "        predictions_df = predictions.select(col(\"pickup_date\"), col(\"label\").alias(\"actual\"), col(\"prediction\").alias(\"predicted\"))\n",
    "        \n",
    "        # Add a column for the borough\n",
    "        predictions_df = predictions_df.withColumn(\"borough\", lit(borough))\n",
    "        \n",
    "        # Append to the list\n",
    "        predictions_list_rf.append(predictions_df)\n",
    "        \n",
    "        # Store RMSE results\n",
    "        results_rf.append((borough, rmse))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing borough {borough}: {e}\")\n",
    "\n",
    "# Combine all the DataFrames in the list into one DataFrame\n",
    "if predictions_list_rf:\n",
    "    rf_results = predictions_list_rf[0]\n",
    "    for df in predictions_list_rf[1:]:\n",
    "        rf_results = rf_results.union(df)\n",
    "\n",
    "    # Show the combined DataFrame\n",
    "    rf_results.show()\n",
    "else:\n",
    "    print(\"No predictions were made.\")\n",
    "\n",
    "# Print RMSE results\n",
    "for borough, rmse in results_rf:\n",
    "    print(f\"Borough: {borough}, RMSE: {rmse}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.4 ('.venv1': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc19f976deae78a4b8dd1c0db37710e451de00dcd2e448b10d5869a2fb538e25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
